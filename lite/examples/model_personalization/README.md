# TensorFlow Lite Example On-device Model Personalization

This example illustrates a way of personalizing a TFLite model
on-device without sending any data to the server. It builds
on top of existing TFLite functionality, and can be adapted
for various tasks and models.

See more in [TensorFlow Blog: Example on-device model personalization with TensorFlow Lite](https://blog.tensorflow.org/2019/12/example-on-device-model-personalization.html)

## Quickstart

Pre-requisites:

- [Android Studio](https://developer.android.com/studio).
- [Python 3.5+](https://www.python.org/downloads/).
- [`virtualenv`](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/#installing-virtualenv)
  (usually comes preinstalled with Python).
- Physical Android device with camera.

### Install and run the application

You can either build and run the application inside Android Studio
or run command line to do so.

If you want to use Android Studio, first import the
project into Android Studio (point it to the top-level `build.gradle`
file), connect your Android device to your machine, and use the
`Run` button in Android Studio. If the `Run` button is inactive,
first add an Android application build configuration for the `app`
module.

If you want to build and install on Linux directly, you can first
switch to the `android` folder, then execute

```shell
gradle wrapper
./gradlew build
```

Then you can run the following command to install the apk

```shell
adb install ./app/build/outputs/apk/debug/app-debug.apk
```

If you need to install gradle, you can reference this [link](https://docs.gradle.org/current/userguide/installation.html)

The running application will look like

![Screenshot from the app](app_screenshot.png)

You should now have the app started on your device. The buttons
at the bottom of the screen correspond to classes that the app
learns to distinguish between. Initially, the confidence scores
(numbers on the buttons) for all classes will be either random
or constant depending on the model.

To train the classifier, switch to "Capture" mode in the top-right
corner, and take some photos for each class. To take a photo and
associate it with a class, press the corresponding class button.
You need to take at least 20 photos, but taking more photos, ideally
with different background and/or object orientation, will greatly
improve accuracy.

After collecting the sample photos, the "Train" button should
become active. Press it, and wait for a few seconds until the loss
goes down. Then press "Pause", and switch back to the inference
mode in the top-right corner.  The classifier should now attempt
to predict the class of the camera input class in real time.

## Structure

There are three main parts of this project:

- Converter: a Python library and CLI that allows you to
  define and generate your personalizable model. The code
  lives under `converter` directory.

- Android library: a library that allows you to use models
  generated by the convert from an Android app. The code
  lives under `android/transfer_api` directory in a separate
  Gradle module.

- Android classifier app: an application that illustrates
  how to use the model personalization library. The code lives
  under `android/app`.

## Full workflow

This section describes how you can apply on-device model
personalization towards solving a custom task.

### Converting the model using the CLI

First, you need to pick the models that you want to use with model
personalization. A TFLite on-device personalization model has two parts:
the base model, which is typically trained for a generic data-rich task,
and the head model, which will be trained on device. Base model weights
are fixed during conversion, and cannot be changed later. You need
to pick two models for both parts respectively.

Instead of preparing TensorFlow models on your own, you may use one of the
included alternative model "configurations". Included are:

- `bases.MobileNetV2Base`, which downloads a pre-trained MobileNetV2 model,
  which is a good fit for image recognition tasks.

- `heads.SoftmaxClassifierHead`, which creates a model with a
  single dense layer followed by softmax activation. This provides
  additional benefits compared to defining the model on your own,
  refer to the "TensorFlow Lite select ops dependency" section below.

See the [Python API](#converting-the-model-using-the-python-api) section
for an exhaustive list of included configurations.

To generate a simple personalizable model for image recognition,
do the following from the `converter` directory:

```sh
# Create a virtualenv. This step is optional but recommended.
virtualenv venv

# Activate the created virtualenv.
source venv/bin/activate

# Install the converter.
pip install -e .

# Convert the model.
tflite-transfer-convert \
  --base_mobilenetv2 \
  --head_softmax \
  --num_classes=4 \
  --train_batch_size=20 \
  --optimizer=sgd \
  --sgd_learning_rate=0.0003 \
  --out_model_dir=mobilenet_softmax_model
```

The model should be now generated in the `mobilenet_softmax_model`
directory.

The more flexible option is to prepare the models you want to
use as base and head as TensorFlow SavedModels.

If you are using Keras and Tensorflow 2.x use `tf.keras.models.save_model`, for the legacy models using Keras and TensorFlow 1.x use `tf.keras.experimental.export_saved_model`.


Suppose your SavedModels are in directories `model_A` and `model_B`.
Then by executing the following a personalizable model will be
generated under `custom_model`.

```sh
# Assuming that we are in a virtualenv with the converter.
tflite-transfer-convert \
  --base_model_dir=model_A \
  --head_model_dir=model_B \
  --num_classes=4 \
  --train_batch_size=20 \
  --optimizer=sgd \
  --sgd_learning_rate=0.0003 \
  --out_model_dir=custom_model
```

### Configuring the optimizer

Besides base and head models, you can also configure the optimizer
that is going to be used on-device for training. Currently, the
only supported optimizers are SGD and Adam. Use `--optimizer=sgd`
or `--optimizer=adam` to select one of them from the CLI. SGD
also requires a `--sgd_learning-rate` flag.

### Converting the custom Keras model using the Python API

You can also use the Python API instead of the CLI to generate
custom Keras models. There are some configurations (e.g. `heads.KerasModelHead`)
that are only available from the Python API.

The current version of TFLiteTransferConverter outputs a custom Keras model
with FlexOps. So, the Android app should include a build rule for TFLite
FlexDelegate. See TensorFlow Select Ops dependency section below.

The exhaustive list of available configurations is as follows:

- For the base model:
  - `bases.SavedModelBase`: loads a model from disk.
  - `bases.MobileNetV2Base`: downloads a MobileNetV2 model pretrained
    on ImageNet. Refer to docstrings for available configuration options.
- For the head model:
  - `heads.SoftmaxClassifierHead`: single fully-connected layer + softmax
    activation. Refer to docstrings for available configuration options.
  - `heads.SavedModelHead`: loads a model from disk.
  - `heads.KerasModelHead`: uses a Keras model.
- For the optimizer:
  - `optimizers.SGD`
  - `optimizers.Adam`

For example, here is how you can convert a Keras head model directly:

```python
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.regularizers import l2
from tfltransfer import bases
from tfltransfer import heads
from tfltransfer import optimizers
from tfltransfer.tflite_transfer_converter import TFLiteTransferConverter

base = bases.MobileNetV2Base(image_size=224)
head = tf.keras.Sequential([
    layers.Flatten(input_shape=(7, 7, 1280)),
    layers.Dense(
        units=32,
        activation='relu',
        kernel_regularizer=l2(0.01),
        bias_regularizer=l2(0.01)),
    layers.Dense(
        units=4,
        activation='softmax',
        kernel_regularizer=l2(0.01),
        bias_regularizer=l2(0.01)),
])

# Optimizer is ignored by the converter! See docs.
head.compile(loss='categorical_crossentropy', optimizer='sgd')

converter = TFLiteTransferConverter(4,
                                    base,
                                    heads.KerasModelHead(head),
                                    optimizers.SGD(3e-2),
                                    train_batch_size=20)

converter.convert_and_save('custom_keras_model')
```

### Using the converted model in your app

Copy the generated model directory to your app's `assets` directory,
and add the following lines to your `build.gradle`:

```groovy
android {
  aaptOptions {
    noCompress "tflite"
  }
}

dependencies {
  // Assuming you copied the Android library from this project to your project.
  implementation project(path: ':transfer_api')

  implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly') { changing = true }
  // Please uncomment the following line if you use a custom Keras model.
  // (see TensorFlow Select Ops dependency section below).
  // implementation('org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly') { changing = true }
}
```

To load the model in your Java code, do the following:

```java
TransferLearningModel model =
    new TransferLearningModel(
        new AssetModelLoader(getActivity(), "path/to/your/model/under/assets"),
        Arrays.asList("Class A", "Class B", "Class C", "Class D"));
```

Refer to JavaDocs and
[the app from this project](android/app/src/main/java/org/tensorflow/lite/examples/transfer)
for usage examples.

For example, if you want to use your own personalizable model, then you need to
comment out the last three lines in `doLast{}`.

```groovy
task downloadModel(type: DefaultTask) {
    doFirst {
        println "Downloading and unpacking the model..."
        mkdir project.buildDir
    }

    doLast {
        ant.mkdir(dir: modelTargetLocation)
        // If you want to use your own models rather than pre-built models,
        // comment out the following three lines.
        ant.get(src: modelUrl, dest: modelArchivePath)
        ant.unzip(src: modelArchivePath, dest: modelTargetLocation)
        ant.delete(file: modelArchivePath)
    }
}
```

## TensorFlow Select Ops dependency for custom Keras models.

There is an important difference between `heads.SoftmaxClassifierHead` and
the rest of the head configurations. `SoftmaxClassifier` is the only
configuration that can work with pure TensorFlow Lite, without any
additional operators. This is possible because the training graph is
hand-written, instead of being generated by
[`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients).
For all other head configurations,
[Select TensorFlow operators](https://www.tensorflow.org/lite/guide/ops_select)
are required.

To use select operators in your project, you currently need to build an
AAR that supports them yourself, following the instructions
[here](https://www.tensorflow.org/lite/guide/ops_select#android_aar).
We provide a pre-built artifact `tensorflow-lite-select-tf-ops:0.0.0-nightly`.

Select ops functionality allows to use a larger set of core TensorFlow operators
in TensorFlow Lite models. Within these operators are some gradient
operators that are required to convert graphs produced by `tf.gradients`.
The cost for using them is higher binary size (an increase of about 6 MiB),
and some initialization time overhead.

## How it works

Model personalization pipeline is implemented on top of existing
TensorFlow Lite functionality. It provides a higher-level abstraction of a
trainable model and maps it to a lower-level representation that involves
multiple “physical” TensorFlow Lite models. Different logical routines
like training and inference correspond to different physical models, and
logically mutable model parameters are managed by the library layer, which
passes them as inputs and outputs of these physical models.

This might sound inefficient, but by carefully using
[direct buffers](https://docs.oracle.com/javase/7/docs/api/java/nio/ByteBuffer.html)
in Java, the number of unnecessary copies is kept as low as possible. Since
actually running the model is the most expensive part in terms of
execution time, the overhead introduced by the library layer should be
negligible.
